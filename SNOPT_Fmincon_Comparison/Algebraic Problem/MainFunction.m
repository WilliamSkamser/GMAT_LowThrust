clear, clc, format longg

%Design Variable 
% initial guess
x= [2;2;2];
%lower and upper bounds
xlow = [-inf;-inf;-inf];
xupp = [inf; inf; inf];

%Objective Function
%bounds on objective function
Flow=zeros(3, 1);
Fupp=zeros(3, 1);
Flow(1) = -inf;
Fupp(1) = +Inf;
%bounds on equality constraints
Flow(2) = 0.0;
Fupp(2) = 0.0;
%bounds of inequality constraints
Flow(3) = -inf;
Fupp(3) = 0;

%Multiplier and state of design variable x and function F


xmul = zeros(3, 1); %Lagrange multipliers
Fmul = zeros(3, 1);

%xstate vector is used to provide information about the optimality and feasibility of the solution.
%0: The variable is free, i.e., it is not fixed at a particular value and is allowed to vary during the optimization.
%1: The variable is fixed, i.e., it is constrained to a particular value and is not allowed to vary during the optimization.
%2: The variable is at its lower bound, i.e., it has reached the lower bound and cannot take a smaller value.
%3: The variable is at its upper bound, i.e., it has reached the upper bound and cannot take a larger value.
%4: The variable is superbasic, i.e., it is a slack variable and is not bound to a particular value.
xstate = zeros(3, 1);
%0: The constraint is inactive, i.e., it is not binding and is not affecting the optimization.
%1: The constraint is binding, i.e., it is active and is affecting the optimization.
%2: The constraint is at its lower bound, i.e., it has reached the lower bound and cannot take a smaller value.
%3: The constraint is at its upper bound, i.e., it has reached the upper bound and cannot take a larger value.
Fstate = zeros(3, 1);

%%Fmincon
optionsFMC=optimoptions('fmincon','Algorithm', 'sqp','Display','iter');
[xOpt,fOpt,exitflag,output2,lambda]=fmincon("objFuncFMC",x,[],[],[],[],...
    xlow,xupp,"nlconsFMC",optionsFMC);
[c,ceq]=nlconsFMC(xOpt);
fprintf("\n\n");

%% SNOPT
%SNOPT Options
snscreen on;  
%snset("Minimize")%set to minimize
snsummary('SNOPt_summary.txt');%create summary file
snseti('Time limit',10) %Sets time limit to 1 day (in seconds)
snseti('Major iteration limit', 1000); %This Works
% tolerance values, 1e-6 by default 
%snseti("Optimality tolerance type", 1);  % Set the optimality tolerance type to "relative", 0 for absolute
%snseti("Feasibility tolerance type", 1);
snsetr('Major feasibility tolerance',1e-6); %For some reason switching between "" and '' makes a difference 
snsetr('Major optimality tolerance',1e-6);
snsetr('Minor feasibility tolerance',1e-6);
snsetr('Minor optimality tolerance',1e-6);
%snseti('Step size',10000);
%snsetr('Line search tolerance',1e-7); %Default is 1e-4
%snseti('Line search algorithm', 0)%Backtracking line search
%snseti('Line search algorithm', 1)%Cubic interpolation line search
%snseti('Line search algorithm', 2)%Quadratic interpolation line search
%snseti('Line search algorithm', 3)%More-Thuente line search
%snseti('Derivative option', 1);  % Set the derivative option to central differences, Forward is 0

ObjAdd =0; %Add value to objective Row
ObjRow =1; %Tell the Optimizer which row of F is the objective function


tic
[x,F,inform,xmul,Fmul,xstate,Fstate,output]= ...
    snopt( x, xlow, xupp, xmul, xstate, Flow, Fupp, Fmul, Fstate, 'objFuncSNOPT', ObjAdd, ObjRow);
toc
[FOut, G] = objFuncSNOPT(x);

fprintf("\n\n");
fprintf("Fmincon x output is \n")
disp(xOpt)
fprintf("Fmincon ObjFunction output is %d \n",objFuncFMC(xOpt))
fprintf("Fmincon Inequality Constraint output is %d \n",c)
fprintf("Fmincon Equality Constraint output is %d \n\n",ceq)
fprintf("SNOPT x output is \n")
disp(x)
fprintf("SNOPT ObjFunction output is %d \n",FOut(1))
fprintf("SNOPT Inequality Constraint output is %d \n",FOut(3))
fprintf("SNOPT Equality Constraint output is %d \n",FOut(2))
%% SNOPT Options
%{
Some specific options that can be set using the snset function include:

Major feasibility tolerance: This option determines the accuracy with which the constraints of the optimization problem are satisfied.
Major optimality tolerance: This option determines the accuracy with which the optimality conditions of the optimization problem are satisfied.
Minor feasibility tolerance: This option determines the accuracy with which the constraints of the optimization problem are satisfied during the minor iterations of the optimization algorithm.
Minor optimality tolerance: This option determines the accuracy with which the optimality conditions of the optimization problem are satisfied during the minor iterations of the optimization algorithm.
Iteration limit: This option sets the maximum number of iterations that the optimization algorithm is allowed to run.
Print level: This option controls the amount of output generated by the optimization algorithm. A print level of 0 suppresses all output, while a print level of 1 generates minimal output, and a print level of 2 or higher generates increasing amounts of detailed output.



Some specific options that can be set using the snseti function include:

Derivative option: This option controls the method used to approximate the derivatives required by the optimization algorithm. Possible values include finite differences, analytic derivatives, and a hybrid approach that combines both methods.
Print file: This option controls where the output generated by the optimization algorithm is printed. Possible values include the screen, a file, or both.
Summation: This option controls how the optimization algorithm handles the summation of quantities that may be very small or very large.
Line search algorithm: This option controls the line search algorithm used by the optimization algorithm to find a suitable step size at each iteration. Possible values include the More-Thuente algorithm, the Powell algorithm, and the Wolfe-Powell algorithm.

    Line Search Algotithms:

Backtracking line search: This is the default line search algorithm in SNOPT. It uses a backtracking approach to find the optimal step size in each iteration.

Cubic interpolation line search: This line search algorithm uses cubic interpolation to find the optimal step size in each iteration. It is generally faster and more accurate than the backtracking line search, but may be less robust in some cases.

Quadratic interpolation line search: This line search algorithm uses quadratic interpolation to find the optimal step size in each iteration. It is generally faster than the backtracking line search, but may be less accurate and less robust in some cases.

More-Thuente line search: This line search algorithm is based on the nonmonotone Armijo rule and uses a backtracking approach to find the optimal step size in each iteration. It is generally more robust than the backtracking line search, but may be slower in some cases.




Some specific options that can be set using the snsetr function include:

Major feasibility tolerance: This option determines the accuracy with which the constraints of the optimization problem are satisfied.
Major optimality tolerance: This option determines the accuracy with which the optimality conditions of the optimization problem are satisfied.
Minor feasibility tolerance: This option determines the accuracy with which the constraints of the optimization problem are satisfied during the minor iterations of the optimization algorithm.
Minor optimality tolerance: This option determines the accuracy with which the optimality conditions of the optimization problem are satisfied during the minor iterations of the optimization algorithm.
Step size: This option controls the initial step size used by the optimization algorithm.
Line search tolerance: This option determines the accuracy with which the line search algorithm tries to satisfy the sufficient decrease condition.

snget command->
snsetr('Major feasibility tolerance', 1e-6);
double tolerance = sngetr('Major feasibility tolerance');

Time limit option using the snsetr function to specify the maximum time allowed
snsetr('Time limit',4)



The snspec function takes as input a number of arguments that specify the problem specification. These arguments include:

n: The number of variables in the optimization problem.
neF: The number of nonlinear equality constraints in the optimization problem.
neG: The number of nonlinear inequality constraints in the optimization problem.
nxName: The length of the xNames array, which specifies the names of the variables in the optimization problem.
xNames: An array of strings specifying the names of the variables in the optimization problem.
nFName: The length of the FNames array, which specifies the names of the constraints in the optimization problem.
FNames: An array of strings specifying the names of the constraints in the optimization problem.
objRow: The row index of the objective in the constraint matrix.
ObjAdd: A constant value to be added to the objective function.
For example, you can use the snspec function to specify a problem with 3 variables, 2 nonlinear equality constraints, and 1 nonlinear inequality constraint like this:

Copy code
char **xNames = (char**) malloc(3*sizeof(char*));
xNames[0] = "x1";
xNames[1] = "x2";
xNames[2] = "x3";

char **FNames = (char**) malloc(3*sizeof(char*));
FNames[0] = "eq1";
FNames[1] = "eq2";
FNames[2] = "ineq1";

snspec(3, 2, 1, 3, xNames, 3, FNames, 0, 0.0);
In this example, the snspec function is used to specify a problem with 3 variables, whose names are given by the xNames array, and 3 constraints, whose names are given by the FNames array. The objective function is specified as the first row of the constraint matrix (objRow=0), and no constant value is added to the objective (ObjAdd=0.0).




%}
